# -*- coding: utf-8 -*-
"""WHObert2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fQvu8HmgPfRjKsh_dDa3nYuULWbAP4y_
"""

!nvidia-smi

!pip install pytorch-lightning --quiet
!pip install transformers --quiet

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np

from tqdm.auto import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup

import pytorch_lightning as pl
# from pytorch_lightning.metrics.functional import f1,auroc
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, multilabel_confusion_matrix

import seaborn as sns
from pylab import rcParams
import matplotlib.pyplot as plt
from matplotlib import rc

# %matplotlib inline
# %config InlineBackend.figure_format='retina'

RANDOM_SEED = 42

sns.set(style='whitegrid', palette='muted', font_scale=1.2)
HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]
sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))
rcParams['figure.figsize'] = 12, 8

pl.seed_everything(RANDOM_SEED)

from google.colab import drive
drive.mount('/content/drive')

import re

data = pd.read_csv("/content/drive/MyDrive/fbposts/combined.csv")
li=[]
x=pd.get_dummies(data['tag']).values
pred1=pd.read_csv('/content/drive/MyDrive/fbposts/post1/predict.csv')
data['text'] = data['text'].apply(lambda x: x.lower())

data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

cols=['A','B','C','D','E']
df2=pd.DataFrame(x,columns=cols)
data[cols]=x
# print(data)
df=data[['text','A','B','C','D','E']]
print(df)



import nltk
from keras.preprocessing.text import Tokenizer
from nltk.corpus import stopwords
nltk.download('stopwords')
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import pandas as pd

feature = 3088
tokenizer = Tokenizer(num_words=feature, filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', split=' ')

tokenizer.fit_on_texts(df['text'].values)
word_index=tokenizer.word_index
for word in stopwords.words('english'):
  try:
    word_index.pop(word)
  except:
    continue
token=[]
words=''
for val in word_index:
  token.append(val)

for i in range(len(token)):
        
     
    words += " ".join(token)+" "
 
wordcloud = WordCloud(width = 800, height = 800,background_color ='white',colormap='Reds', stopwords=STOPWORDS,min_font_size = 10).generate(words)
 
# plot the WordCloud image                      
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)
 
plt.show()

data2 = pd.read_csv("/content/drive/MyDrive/fbposts/post2/postframe2.csv")
li=[]
y=pd.get_dummies(data2['tag']).values
# predf=pd.read_csv('/content/drive/MyDrive/fbposts/post1/predict.csv')
data2['text'] = data2['text'].apply(lambda x: x.lower())
pred2=pd.read_csv('/content/drive/MyDrive/fbposts/post2/predict2.csv')
data2['text'] = data2['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

dftemp=pd.DataFrame(y,columns=cols)
data2[cols]=y
# print(data)
df2=data2[['text','A','B','C','D','E']]
print(df2)

data3 = pd.read_csv("/content/drive/MyDrive/fbposts/post3/postframe3.csv")
li=[]
z=pd.get_dummies(data3['tag']).values
# predf=pd.read_csv('/content/drive/MyDrive/fbposts/post1/predict.csv')
data3['text'] = data3['text'].apply(lambda x: x.lower())
pred3=pd.read_csv('/content/drive/MyDrive/fbposts/post3/predict.csv')
data3['text'] = data3['text'].apply((lambda x: re.sub('[^a-zA-z0-9\s]','',x)))

dftemp=pd.DataFrame(z,columns=cols)
data3[cols]=z
# print(data)
df3=data3[['text','A','B','C','D','E']]
print(df3)

train_df, val_df = train_test_split(df, test_size=0.2)
print(train_df.shape, val_df.shape)

# train_df2, val_df2 = train_test_split(df2, test_size=0.2)
# print(train_df2.shape, val_df2.shape)

# train_df3, val_df3 = train_test_split(df3, test_size=0.2)
# print(train_df3.shape, val_df3.shape)

LABEL_COLUMNS = df.columns.tolist()[1:]

Bmodel = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(Bmodel)

maxt = 75

class fbdataset(Dataset):

  def __init__(
    self, 
    data: pd.DataFrame, 
    tokenizer: BertTokenizer, 
    max_token_len: int = 75
  ):
    self.tokenizer = tokenizer
    self.data = data
    self.max_token_len = max_token_len
    
  def __len__(self):
    return len(self.data)

  def __getitem__(self, index: int):
    comments = self.data.iloc[index]

    text = comments.text
    labels = comments[LABEL_COLUMNS]

    encoding = self.tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=self.max_token_len,
      return_token_type_ids=False,
      padding="max_length",
      truncation=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

    return dict(
      text=text,
      input_ids=encoding["input_ids"].flatten(),
      attention_mask=encoding["attention_mask"].flatten(),
      labels=torch.FloatTensor(labels)
    )

bert_model = BertModel.from_pretrained(Bmodel, return_dict=True)

class module(pl.LightningDataModule):

  def __init__(self, train_df, test_df, tokenizer, batch_size=8, max_token_len=128):
    super().__init__()
    self.batch_size = batch_size
    self.train_df = train_df
    self.test_df = test_df
    self.tokenizer = tokenizer
    self.max_token_len = max_token_len

  def setup(self, stage=None):
    self.train_dataset = fbdataset(
      self.train_df,
      self.tokenizer,
      self.max_token_len
    )

    self.test_dataset = fbdataset(
      self.test_df,
      self.tokenizer,
      self.max_token_len
    )

  def train_dataloader(self):
    return DataLoader(
      self.train_dataset,
      batch_size=self.batch_size,
      shuffle=True,
      num_workers=2
    )

  def val_dataloader(self):
    return DataLoader(
      self.test_dataset,
      batch_size=self.batch_size,
      num_workers=2
    )

  def test_dataloader(self):
    return DataLoader(
      self.test_dataset,
      batch_size=self.batch_size,
      num_workers=2
    )

epoch = 10
BATCH_SIZE = 12

data_module1 = module(
  train_df,
  val_df,
  tokenizer,
  batch_size=BATCH_SIZE,
  max_token_len=75
)

import torch
import torchmetrics
accuracy= torchmetrics.Accuracy()

class tag(pl.LightningModule):

  def __init__(self, n_classes: int, n_training_steps=None, n_warmup_steps=None):
    super().__init__()
    self.bert = BertModel.from_pretrained(Bmodel, return_dict=True)
    self.classifier = nn.Linear(self.bert.config.hidden_size, n_classes)
    self.n_training_steps = n_training_steps
    self.n_warmup_steps = n_warmup_steps
    self.criterion = nn.BCELoss()

  def forward(self, input_ids, attention_mask, labels=None):
    output = self.bert(input_ids, attention_mask=attention_mask)
    output = self.classifier(output.pooler_output)
    output = torch.sigmoid(output)    
    loss = 0
    if labels is not None:
        loss = self.criterion(output, labels)
    return loss, output

  def training_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("train_loss", loss, prog_bar=True, logger=True)
    return {"loss": loss, "predictions": outputs, "labels": labels}

  def validation_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("val_loss", loss, prog_bar=True, logger=True)
    return loss

  def test_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("test_loss", loss, prog_bar=True, logger=True)
    return loss

  def training_epoch_end(self, outputs):
    
    labels = []
    predictions = []
    for output in outputs:
      for out_labels in output["labels"].detach().cpu():
        labels.append(out_labels)
      for out_predictions in output["predictions"].detach().cpu():
        predictions.append(out_predictions)

    labels = torch.stack(labels).int()
    predictions = torch.stack(predictions)

    for i, name in enumerate(LABEL_COLUMNS):
      class_roc_auc = auroc(predictions[:, i], labels[:, i])
      self.logger.experiment.add_scalar(f"{name}_roc_auc/Train", class_roc_auc, self.current_epoch)


  def configure_optimizers(self):

    optimizer = AdamW(self.parameters(), lr=2e-5)

    scheduler = get_linear_schedule_with_warmup(
      optimizer,
      num_warmup_steps=self.n_warmup_steps,
      num_training_steps=self.n_training_steps
    )

    return dict(
      optimizer=optimizer,
      lr_scheduler=dict(
        scheduler=scheduler,
        interval='step'
      )
    )



steps_per_epoch=len(train_df) // BATCH_SIZE
total_training_steps = steps_per_epoch * epoch

warmup_steps = total_training_steps // 5
warmup_steps, total_training_steps

model1 = tag(
  n_classes=len(LABEL_COLUMNS),
  n_warmup_steps=warmup_steps,
  n_training_steps=total_training_steps 
)

callback = ModelCheckpoint(
  dirpath="checkpoints",
  filename="best-checkpoint",
  save_top_k=1,
  verbose=True,
  monitor="val_loss",
  mode="min"
)

trainer1 = pl.Trainer(
  
  callbacks=callback,
  
  max_epochs=epoch,
  gpus=1,
  progress_bar_refresh_rate=30
)

trainer1.fit(model1, data_module1)

trainer1.test()

trained_model1 = ToxicCommentTagger.load_from_checkpoint(
  trainer1.checkpoint_callback.best_model_path,
  n_classes=len(LABEL_COLUMNS)
)
trained_model1.eval()
trained_model1.freeze()

class final(Dataset):

  def __init__(
    self, 
    data: pd.DataFrame, 
    tokenizer: BertTokenizer, 
    max_token_len: int = 75
  ):
    self.tokenizer = tokenizer
    self.data = data
    self.max_token_len = max_token_len
    
  def __len__(self):
    return len(self.data)

  def __getitem__(self, index: int):
    comments = self.data.iloc[index]

    text = comments.text
    

    encoding = self.tokenizer.encode_plus(
      text,
      add_special_tokens=True,
      max_length=self.max_token_len,
      return_token_type_ids=False,
      padding="max_length",
      truncation=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

    return dict(
      text=text,
      input_ids=encoding["input_ids"].flatten(),
      attention_mask=encoding["attention_mask"].flatten(),
      
    )

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
trained_model1 = trained_model1.to(device)

val_dataset1 = ToxicCommentsDataset(
  val_df,
  tokenizer,
  max_token_len=75
)

predictions = []
labels = []

for item in tqdm(val_dataset1):
  _, prediction = trained_model1(
    item["input_ids"].unsqueeze(dim=0).to(device), 
    item["attention_mask"].unsqueeze(dim=0).to(device)
  )
  predictions.append(prediction.flatten())
  labels.append(item["labels"].int())

predictions = torch.stack(predictions).detach().cpu()
labels = torch.stack(labels).detach().cpu()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(accuracy(predictions, labels))

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
trained_model1 = trained_model1.to(device)

val_dataset1 = final(
  pred1,
  tokenizer,
  max_token_len=75
)

predict1 = []


# lim1= len(predf['text'])

for item in tqdm(val_dataset1):
  _, prediction = trained_model1(
    item["input_ids"].unsqueeze(dim=0).to(device), 
    item["attention_mask"].unsqueeze(dim=0).to(device)
  )
  predict1.append(prediction.flatten())
  

predict1= torch.stack(predict1).detach().cpu()
print(predict1)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
trained_model1 = trained_model1.to(device)

val_dataset2 = final(
  pred2,
  tokenizer,
  max_token_len=75
)

predict2 = []


# lim3= len(pred2['text'])

for item in tqdm(val_dataset2):
  _, prediction = trained_model1(
    item["input_ids"].unsqueeze(dim=0).to(device), 
    item["attention_mask"].unsqueeze(dim=0).to(device)
  )
  predict2.append(prediction.flatten())
  

predict2 = torch.stack(predict2).detach().cpu()
print(predict2)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
trained_model1 = trained_model1.to(device)

val_dataset3 = final(
  pred3,
  tokenizer,
  max_token_len=75
)

predict3 = []


lim3= len(pred3['text'])

for item in tqdm(val_dataset3):
  _, prediction = trained_model1(
    item["input_ids"].unsqueeze(dim=0).to(device), 
    item["attention_mask"].unsqueeze(dim=0).to(device)
  )
  predict3.append(prediction.flatten())
  

predict3 = torch.stack(predict3).detach().cpu()
print(predict3)

tags1=[]
lim1=len(pred1['text'])
lim2=len(pred2['text'])
lim3=len(pred3['text'])
for i in range(0,lim1):
    
    if(predict1[i][0] > predict1[i][1] and predict1[i][0] > predict1[i][2] and predict1[i][0]>predict1[i][3] and predict1[i][0]>predict1[i][4]):
        
        tags1.append('A')
    elif(predict1[i][1] > predict1[i][0] and predict1[i][1] > predict1[i][2] and predict1[i][1]>predict1[i][3] and predict1[i][1]>predict1[i][4]):
        
        tags1.append('B')
    elif(predict1[i][2] > predict1[i][0] and predict1[i][2] > predict1[i][1] and predict1[i][2]>predict1[i][3] and predict1[i][2]>predict1[i][4]):
        
        tags1.append('C')
    elif(predict1[i][3] > predict1[i][0] and predict1[i][3] > predict1[i][1] and predict1[i][3]>predict1[i][2] and predict1[i][3]>predict1[i][4]):
        
        tags1.append('D')
    elif(predict1[i][4] > predict1[i][0] and predict1[i][4] > predict1[i][1] and predict1[i][4]>predict1[i][2] and predict1[i][4]>predict1[i][3]):
        
        tags1.append('E')

print(tags1)

tags2=[]

for i in range(0,lim2):
    
    if(predict2[i][0] > predict2[i][1] and predict2[i][0] > predict2[i][2] and predict2[i][0]>predict2[i][3] and predict2[i][0]>predict2[i][4]):
        
        tags2.append('A')
    elif(predict2[i][1] > predict2[i][0] and predict2[i][1] > predict2[i][2] and predict2[i][1]>predict2[i][3] and predict2[i][1]>predict2[i][4]):
        
        tags2.append('B')
    elif(predict2[i][2] > predict2[i][0] and predict2[i][2] > predict2[i][1] and predict2[i][2]>predict2[i][3] and predict2[i][2]>predict2[i][4]):
        
        tags2.append('C')
    elif(predict2[i][3] > predict2[i][0] and predict2[i][3] > predict2[i][1] and predict2[i][3]>predict2[i][2] and predict2[i][3]>predict2[i][4]):
        
        tags2.append('D')
    elif(predict2[i][4] > predict2[i][0] and predict2[i][4] > predict2[i][1] and predict2[i][4]>predict2[i][2] and predict2[i][4]>predict2[i][3]):
        
        tags2.append('E')

print(tags2)

tags3=[]

for i in range(0,lim3):
    
    if(predict3[i][0] > predict3[i][1] and predict3[i][0] > predict3[i][2] and predict3[i][0]>predict3[i][3] and predict3[i][0]>predict3[i][4]):
        
        tags3.append('A')
    elif(predict3[i][1] > predict3[i][0] and predict3[i][1] > predict3[i][2] and predict3[i][1]>predict3[i][3] and predict3[i][1]>predict3[i][4]):
        
        tags3.append('B')
    elif(predict3[i][2] > predict3[i][0] and predict3[i][2] > predict3[i][1] and predict3[i][2]>predict3[i][3] and predict3[i][2]>predict3[i][4]):
       
        tags3.append('C')
    elif(predict3[i][3] > predict3[i][0] and predict3[i][3] > predict3[i][1] and predict3[i][3]>predict3[i][2] and predict3[i][3]>predict3[i][4]):
        
        tags3.append('D')
    elif(predict3[i][4] > predict3[i][0] and predict3[i][4] > predict3[i][1] and predict3[i][4]>predict3[i][2] and predict3[i][4]>predict3[i][3]):
        
        tags3.append('E')

print(tags3)

dfchart1=pd.DataFrame(tags1,columns=['tags'])
print(dfchart1)
dfchart2=pd.DataFrame(tags2,columns=['tags'])
print(dfchart2)

dfchart3=pd.DataFrame(tags3,columns=['tags'])
print(dfchart3)

ct1 = dfchart1["tags"].value_counts()
print(ct1)
vals1=[]
for i in range(0,len(ct1)):
  vals1.append(ct1[i])

plt.bar(ct1.keys(), vals1, color ='blue',
        width = 0.4)
plt.xlabel("Tags          T1")
plt.ylabel("Number of Comments")
plt.show()

ct2 = dfchart2["tags"].value_counts()
print(ct2)
vals2=[]
for i in range(0,len(ct2)):
  vals2.append(ct2[i])

plt.bar(ct2.keys(), vals2, color ='blue',
        width = 0.4)
plt.xlabel("Tags       T2")
plt.ylabel("Number of Comments")
plt.show()


ct3 = dfchart3["tags"].value_counts()
print(ct3)
vals3=[]
for i in range(0,len(ct3)):
  vals3.append(ct3[i])

plt.bar(ct3.keys(), vals3, color ='blue',
        width = 0.4)
plt.xlabel("Tags             T3")
plt.ylabel("Number of Comments")
plt.show()

print("AUROC per tag")
for i, name in enumerate(LABEL_COLUMNS):
  tag_auroc = auroc(predictions[:, i], labels[:, i], pos_label=1)
  print(f"{name}: {tag_auroc}")

y_pred = predictions.numpy()
y_true = labels.numpy()

upper, lower = 1, 0

y_pred = np.where(y_pred>0.5, upper, lower)

print(classification_report(
  y_true, 
  y_pred, 
  target_names=LABEL_COLUMNS, 
  zero_division=0
))

# p=0
# percent1=[]
# for i in range(0,len(ct1)):
#   p=(ct1[i]/4420)*100
#   percent1.append(p)

# print(percent1)

import matplotlib.pyplot as plt
import numpy as np
  
  
# Define X and Y variable data

ab = [49.2,74.8,62.4]
de=[12.2,1.8,5.0]
ne=[38,23.2,33.1]
plt.plot(['T1','T2','T3'],ab,color='r',marker='o')
plt.plot(['T1','T2','T3'],ne,color='b',marker='o')
plt.plot(['T1','T2','T3'],de,color='g',marker='o')
plt.xlabel("Time periods")
plt.ylabel("Percent")
plt.legend(["Negative", "Neutral",'Positive'], loc ="upper left")
plt.show()

import matplotlib.pyplot as plt
import numpy as np
  
  
# Define X and Y variable data

a = [28.9,42.08,36.6]
b=[20.3,32.8,25.8]
c=[38,23.2,33.1]
d=[3.3,0.95,1.99]
e=[8.9,0.93,3.04]
plt.plot(['T1','T2','T3'],a,color='red',marker='o')
plt.plot(['T1','T2','T3'],b,color='orange',marker='o')
plt.plot(['T1','T2','T3'],c,color='blue',marker='o')
plt.plot(['T1','T2','T3'],d,color='yellow',marker='o')
plt.plot(['T1','T2','T3'],e,color='green',marker='o')
plt.xlabel("Time periods")
plt.ylabel("Percent")
plt.legend(["A", "B",'C',"D","E"], loc ="upper left")
plt.show()